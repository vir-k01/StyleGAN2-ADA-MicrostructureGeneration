{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN2-ADA-Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRgpkbh2w3D3JOw1QUG3ZB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vir-k01/StyleGAN2-ADA-MicrostructureGeneration/blob/main/StyleGAN2_ADA_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StyleGAN2-ADA Training"
      ],
      "metadata": {
        "id": "1Q7kcx5CV9ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code accompanying the manuscript: \"Quantification of similarity and physical awareness of microstructures generated via Generative Models\" by Sanket Thakre, Vir Karan, Anand Krishna Kanjarla.\n",
        "\n",
        "-Notebook authored by Vir Karan"
      ],
      "metadata": {
        "id": "HU6Fb2Bwiomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The basic idea"
      ],
      "metadata": {
        "id": "Shbqphq_aplH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the code needed to prepare a custom dataset and train a StyleGAN2-ADA model in the Google Colab environment. This notebook clones the official repository by Nvidia (https://github.com/NVlabs/stylegan2-ada) and then calls the scripts from that repo. Relevant changes can be made to run this locally or on a workstation. \n",
        "\n",
        "\n",
        "\n",
        "(Some aspects of this notebook have been adapted from https://github.com/dvschultz/ml-art-colabs/blob/master/Stylegan2_ada_Custom_Training.ipynb)"
      ],
      "metadata": {
        "id": "HUrQdf3bYUzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StyleGAN2-ADA only work with Tensorflow 1. Run the next cell before anything else to make sure we’re using TF1 and not TF2.**"
      ],
      "metadata": {
        "id": "mOfxf38FWEFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS_GH_iNV5L4"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure that a GPU runtime has been selected. GANs require **a lot** of compute to train, so make sure you've got a GPU with atleast 12GB RAM here. In the case of Colab, the default K80 GPU should suffice for smaller datasets, but it is preferable to use a T4 or P100 or better GPU. "
      ],
      "metadata": {
        "id": "3bBPvML_Z3XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "qAellYAmWHAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Colab runtimes are very unreliable wrt stability while training, it is always better to mount Google Drive and save/load all files from there. In this case, we'll also install the StyleGAN repo to drive directly to save time for future runs."
      ],
      "metadata": {
        "id": "SwSE53nMaQaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2J-NFOhMWJiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, run this cell. If you’re already installed the repo, it will skip the installation process and change into the repo’s directory. If you haven’t installed it, it will install all the files necessary."
      ],
      "metadata": {
        "id": "YMAV_fn2WQW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/My Drive/colab-sg2-ada\"):\n",
        "    %cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
        "else:\n",
        "    #install script\n",
        "    %cd \"/content/drive/My Drive/\"\n",
        "    !mkdir colab-sg2-ada\n",
        "    %cd colab-sg2-ada\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada\n",
        "    %cd stylegan2-ada\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets"
      ],
      "metadata": {
        "id": "eew_QJZmWOVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/My Drive/colab-sg2-ada/stylegan2-ada\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git checkout origin/main -- train.py"
      ],
      "metadata": {
        "id": "WiIAvwkjWTK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation"
      ],
      "metadata": {
        "id": "CqSbVNjgWXET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The StyleGAN requires the dataset to be in `.tfrecords` files. Also, the actual images needed to train the model have to be in the RGB format (to be able to use pre-trained models). The dataset (attached with this repo: https://drive.google.com/drive/folders/1Oz2NlycaIfK0-mebNgz5HTTW4_UuVikQ?usp=sharing) has to first be extracted and converted to this format."
      ],
      "metadata": {
        "id": "zXh6Ww6Ia0sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "IWIgiAceWVd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The functions in the next cell are general functions written to extract the microstructural data from the .txt files and save them as np arrays. There are additional arguements present to perform image augmentation (Shifting, Flipping, Rotating, Cropping) as well, however, for this particular work this augmentation is not needed. "
      ],
      "metadata": {
        "id": "mPFeaMrScvpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_store_data(filepath):\n",
        "  readfile = open(filepath, \"r\")\n",
        "  arr1 =[]\n",
        "  for line in readfile:\n",
        "    Type = line.split(\" \")\n",
        "    arr1.append(Type)\n",
        "  arr1 = np.asarray(arr1)\n",
        "  arr1 = arr1.astype(np.float)\n",
        "  arr1 = np.squeeze(arr1)\n",
        "  arr1 = arr1[:, 3][:256*256]\n",
        "  arr1 = np.rot90(np.reshape(arr1, (256,256)))\n",
        "  plt.imshow(arr1, cmap = 'gray')\n",
        "  return arr1\n",
        "\n",
        "def store_array(im, filename):\n",
        "  fil = open(str(filename) + '.txt', 'w')\n",
        "  row, column = im.shape\n",
        "  for y in range(column):\n",
        "      for x in range(row):\n",
        "          pixel = im[y, x]\n",
        "          fil.write(str(x+1) + \" \" + str(y+1) + \" \" + str(pixel) + '\\n')\n",
        "  fil.close()\n",
        "\n",
        "def read_many_disk(Shift, Flip, Rotate, Crop, num_classes, num_images = 50, Angle = 90, width = 128, height =128):\n",
        "    \"\"\" Reads image from disk.\n",
        "        Parameters:\n",
        "        ---------------\n",
        "        Shift        Boolean          Whether to shift the position of the image to the left or right \n",
        "        Flip         Boolean          Whether to flip the image along the horizontal and vertical directions\n",
        "        Rotate       Boolean          Whether to rotate the image by \"Angle\" deg (90deg) C.C.W\n",
        "        Crop         Boolean          Whether to randomly crop the image to a box of size \"width\" by \"height\"\n",
        "\n",
        "        num_images   number of images to read per class\n",
        "        num_class    number of class\n",
        "\n",
        "        Returns:\n",
        "        ----------\n",
        "        images      images array, (N, 256, 256, num_class) to be stored, here N = num_images * num_class\n",
        "        labels      associated meta data, int label (N, 1)\n",
        "    \"\"\"\n",
        "    images, labels = [], []\n",
        "\n",
        "    # Loop over all IDs and read each image in one by one\n",
        "\n",
        "    for class_id in range(num_classes+1):\n",
        "      if class_id:\n",
        "        for image_id in range(num_images+1):\n",
        "          if image_id:\n",
        "            images.append(np.array(read_store_data('/content/drive/MyDrive/GAN/Damage prediction/Class '+ f'{class_id}/'+ f'{image_id}.txt')))\n",
        "\n",
        "    for j in range(num_classes):\n",
        "      for i in range(50):\n",
        "        labels.append(j)\n",
        "    \n",
        "    data = np.asarray(images)\n",
        "    labels = np.asarray(labels)\n",
        "    labels = labels.astype(np.float64)\n",
        "  \n",
        "    X_train_1 = []\n",
        "    y_train_1 = []\n",
        "    X_train_augmented = [image for image in data]\n",
        "    y_train_augmented = [label for label in labels]\n",
        "\n",
        "    for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
        "        for image, label in zip(data, labels):\n",
        "          if Shift:\n",
        "            X_train_augmented.append(shift_image(image, dx, dy))\n",
        "            y_train_augmented.append(label)\n",
        "          if Flip:\n",
        "            X_train_augmented.append(np.fliplr(image))\n",
        "            X_train_augmented.append(np.flipud(image))\n",
        "            y_train_augmented.append(label)\n",
        "            y_train_augmented.append(label)\n",
        "          if Rotate:\n",
        "            for i in [90, 180, 270]:\n",
        "              X_train_augmented.append(rotate(image, angle= i))\n",
        "              y_train_augmented.append(label)\n",
        "\n",
        "    X_train_augmented = np.array(X_train_augmented)\n",
        "    y_train_augmented = np.array(y_train_augmented)\n",
        "    \n",
        "    if Crop:\n",
        "      for image, label in zip(X_train_augmented, y_train_augmented):\n",
        "        for i in range(4):\n",
        "          X_train_1.append(random_crop(image, width, height))\n",
        "          y_train_1.append(label)    \n",
        "      X_train_1 = np.array(X_train_1)\n",
        "      y_train_1 = np.array(y_train_1)\n",
        "      return [X_train_1, y_train_1]  \n",
        "    else:\n",
        "      return [X_train_augmented, y_train_augmented]"
      ],
      "metadata": {
        "id": "FKp7qZe8WZjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = read_many_disk(0, 0, 0, 0, 6)"
      ],
      "metadata": {
        "id": "XDcWUwLaW82S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, do this to convert the grayscale data to RGB (well technically it's still grayscale, but this is done since the model architecture is built for RGB). We're bascially expanding the dimensions of the individual images and saving it back out as .jpeg files. "
      ],
      "metadata": {
        "id": "kAFVzkSlXDnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "for i in range(0, np.shape(X)[0], 1):\n",
        "  im = np.stack((X[i],X[i], X[i]), axis = 2)\n",
        "  imageio.imwrite('/content/drive/MyDrive/GAN/damage_data/'+ f'{i}'+'.jpeg', im)"
      ],
      "metadata": {
        "id": "_zui2BE6W-4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert dataset to .tfrecords"
      ],
      "metadata": {
        "id": "s2liR6h3XLF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: You only need to do this once per dataset. If you have already run this and are returning to conntinue training, skip these cells.**\n",
        "\n",
        "Next we need to convert our image dataset to a format that StyleGAN2-ADA can read from. There are two options here. You can upload your dataset directly to Colab (as a zipped file), or you can upload it to Drive directly and read it from there."
      ],
      "metadata": {
        "id": "XbwpaYGGXL3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that your image dataset is uploaded, we need to convert it to the `.tfrecords` format. Depending on the resolution of your images and how many you have, this can take a while. The final dataset will be stored on Google Drive for easier access in subsequent runs."
      ],
      "metadata": {
        "id": "bMfkVJsSXPdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#update this to the path to your image folder\n",
        "dataset_path = \"/content/drive/MyDrive/GAN/damage_data/\"\n",
        "#give your dataset a name\n",
        "dataset_name = \"dam_data256\"\n",
        "\n",
        "#you don't need to edit anything here\n",
        "!python dataset_tool.py create_from_images ./datasets/{dataset_name} {dataset_path}"
      ],
      "metadata": {
        "id": "vzBNZ5hgXIiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a custom model\n",
        "\n",
        "We’re ready to start training! There are numerous arguments to training, what’s listed below are the most popular options. To see all the options, run the following cell. All the required arguements will be set in the folling cell though."
      ],
      "metadata": {
        "id": "JqGS0kICXWvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --help"
      ],
      "metadata": {
        "id": "_5vHcHyyXXIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since StyleGAN training can take forever on Colab (as per the official repo, it takes >3 days to train from scratch on 8 V100 GPUs!), we'll use a pretrained StyleGAN as the starting point for our model. Download the pre-trained .pkl file, so you can start training on top of that directly. In the case of microstructural data, you can download our trained model (available in the repo). Else, the pre-trained ffhq-1024 or horse-256 are good starting points depending on your dataset's resolution. "
      ],
      "metadata": {
        "id": "ncbnGFcnXh0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-horse-config-f.pkl"
      ],
      "metadata": {
        "id": "56SUt4HcXekK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this name must EXACTLY match the dataset name you used when creating the .tfrecords file\n",
        "dataset_name = \"dam_data256\"\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 1\n",
        "#should the images be mirrored left to right?\n",
        "mirrored = False\n",
        "#should the images be mirrored top to bottom?\n",
        "mirroredY = False\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#augments\n",
        "augs = \"bg\"\n",
        "\n",
        "#\n",
        "# this is the most important cell to update\n",
        "#\n",
        "# running it for the first time? set it to ffhq(+resolution)\n",
        "# resuming? get the path to your latest .pkl file and use that\n",
        "resume_from = '/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results/00045-dam_data256-11gb-gpu-bg-resumecustom/network-snapshot-000020.pkl'\n",
        "\n",
        "#don't edit this unless you know what you're doing :)\n",
        "!python train.py --outdir ./results --snap={snapshot_count} --cfg=11gb-gpu --data=./datasets/{dataset_name} --augpipe={augs} --resume={resume_from} --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --augpipe=\"bg\""
      ],
      "metadata": {
        "id": "sCNMQU3iXk-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### While it’s training...\n",
        "**Once the above cell is running you should be training!**\n",
        "\n",
        "Don’t close this tab! Colab needs to be open and running in order to continue training. Every ~15min or so a new line should get added to your output, indicated its still training. Depending on you `snapshot_count` setting you should see the results folder in your Google drive folder fill with both samples (`fakesXXXXXx.jpg`) and model weights (`network-snapshot-XXXXXX.pkl`). The samples are worth looking at while it trains but don’t get too worried about each individual sample.\n",
        "\n",
        "If you chose a metric, you will also see scores for each snapshot. Don’t obsess over these! they are a guide, it can go up or down slightly for each snapshot. What you want to see is a gradual lowering of the score over time.\n",
        "\n",
        "Once Colab shuts off, you can Reconnect the notebook and re-run every cell from top to bottom. Make sure you update the `resume_from` path to continue training from the latest model. \n",
        "\n",
        "In our case, it took about 6 hours (on a T4 GPU) to get results that were visually similar to the training dataset. "
      ],
      "metadata": {
        "id": "GnBskfxtXrjW"
      }
    }
  ]
}